---
title: "Intergrating Stream Monitoring Data"
output:
  html_document:
    df_print: paged
---
```{r}

#library(xlsx)
library(readxl)
library(openxlsx)
library(httr)
library(tidyverse)
library(data.table)

#Packages to format table
library(knitr)
library(kableExtra)

# packages for downloading and formatting shapefiles & other GIS Data 
library(downloader)
library(rgdal)
library(RCurl)
library(sf)

#Mapping, graphing and visualizations packages 
library(shiny)
library(leaflet)
library(dplyr)
library(leaflet.extras)
library(DT)
library(ggplot2)


```

#Background
Improving data sharing will enable timely access to data, enhance the quality of data, and create clear channels for better management decisions. Healthy aquatic habitat is critical to fishes, aquatic species, and water quality. Across the country long term, large-scale stream habitat monitoring programs collect data for their specific objectives and within their jurisdictional boundaries. Streams cross jurisdictional boundaries and face unprecedented pressure from changing climate, multi-use public lands, and development. To meet these stresses, we need a way to ingratiate data from multiple sources to create indicators of stream conditions across jurisdictional boundaries.

The Pacific Northwest Aquatic Monitoring Partnership (PNAMP) and the USGS facilitated a working group of four large scales, long-term monitoring programs EPA National Aquatic Resources Surveys (NARS), BLM Aquatic Assessment, Inventory, and Monitoring (AIM), and USFS Aquatic and Riparian Effective Monitoring Program (AREMP), and PACFISH ⁄ INFISH Biological Opinion (PIBO) Effectiveness Monitoring Program. Each program answers different management questions. Still, monitor similar stream types, have similar field collection methods, data structure, and produce a subset of the same methods. 

```{r}
#Get and display the program data
github_link <- "https://raw.githubusercontent.com/rascully/Stream-Monitoring-Data-Exchange-Specifications/master/Data/Metadata.xlsx"

temp_file <- tempfile(fileext = ".xlsx")
req <- GET(github_link, 
           # authenticate using GITHUB_PAT
             authenticate(Sys.getenv("GITHUB_PAT"), ""),
             # write result to disk
             write_disk(path = temp_file))
Program_metadata<- read.xlsx(temp_file,1)
unlink(temp_file)

Program_metadata %>% 
  kbl() %>% 
  kable_styling()

```

We design a workflow to get each program’s data from its original format into an integrated data set. To build the combined data set, we document the online location and metadata of the original data sets in ScienceBase, creating a standard searchable, machine-readable documentation that we write code against to pull the information. ScienceBase is an integrated data-sharing platform managed by the U.S. Geological Survey. It provides access to scientific data and other information using a Web Browser, an application program interface, and representational state transfer (REST)-based Web Services (Chase 2016). We write R code to download the EPA NRSA and USFS AREMP data and reformat the data to a flat data format. The BLM AIM data is already flat, so we don’t need to do any data reformatting. Using the data dictionary, crosswalk, and data schema, we write R code to build one data set, integrating data from the three programs with accessible data sets. We publish the data set on ScienceBase. It is searchable, machine-readable, and accessible for download via an API for web portals, mapping tools, and programming packages (R and Python)(Klein et. Al 2019).


#Program Metrics 
We compiled metadata from each of the four programs and facilitated a discussion on metric comparability. We built a controlled vocabulary (property registry?) using previously published literature and expert opinion.    

```{r}

#Get and display the controlled vocabulary
github_link<- "https://raw.githubusercontent.com/rascully/Stream-Monitoring-Data-Exchange-Specifications/master/Tables/ControlledVocabulary.csv"

temp_file <- tempfile(fileext = ".xlsx")
req <- GET(github_link, 
           # authenticate using GITHUB_PAT
             authenticate(Sys.getenv("GITHUB_PAT"), ""),
             # write result to disk
             write_disk(path = temp_file))
controlled_vocabulary <- read.csv(temp_file,1)
unlink(temp_file)

controlled_vocabulary %>%
  group_by(measurementType)%>%
  count("measurementType") %>% 
  kbl() %>% 
  kable_styling()

```

#Reformating Data 

Each program releases its data in different ways. EPA NARS releases data online in a .CSV file every five years after data collection, BLM AIM releases an ArcGIS server updates every year, and USFS AREMP releases a ArcGIS geodatabase every five years. To integrate data sets from different sources the original data sets might need to be reformatted. For this pilot project two of the data sets need to be reformatted and projected. The final data set coordinates will be shared in decimal degrees in WGS84. 

```{r}
CRS<-  "+proj=longlat +datum=WGS84 +no_defs"


```

##EPA Rivers and Streams Data 

EPA collects data across the United States; they share their metric level data here https://www.epa.gov/national-aquatic-resource-surveys/data-national-aquatic-resource-surveys. They release data sets for each two-year collection period, 2004-2005 and 2008-2009, 2013-2014. The 2018-2019 data set is not posted. We will update this when the data are released.

Each data set differs slightly; the data set has different titles, field names for the same metrics, and different formats. We scrape the EPA National Aquatic Resources Survey data page and combines the data sets from multiple years of physical habitat, location, and chemistry data into one flat data set. The EPA NRSA data set produced For each collection of data, 2004-2005 and 2008-2009 we build one containing the information we want to integrate into this analysis and the fields outlined in the data exchange schema.

You can find the metadata and the data used to create one dataset for each year range on ScienceBase


```{r}
setwd("C:/Users/rscully/Documents/Projects/Habitat Data Sharing/2019_2020/Code/Intergrating-Stream-Monitoring-Data-From-Multiple-Programs")

source(paste0(getwd(), "/R/Download and clean EPA NRSA.R"))

NRSA_data <- download_EPA_NRSA()
dim(NRSA_data)[1]

m <- NRSA_data%>% 
  leaflet() %>%
  addTiles() %>%
  addCircles(lng=~LON_DD83, lat= ~LAT_DD83, color="red") 
       
m

```
#USFS AREMP Data 
AREMP published their data as a geodatabase after they release their five-year report here: https://www.fs.fed.us/r6/reo/monitoring/watershed/. The data was last updated in 2015.

You can find the metadata and the data used to create one dataset on ScienceBase:

```{r}
setwd("C:/Users/rscully/Documents/Projects/Habitat Data Sharing/2019_2020/Code/Intergrating-Stream-Monitoring-Data-From-Multiple-Programs")
source(paste0(getwd(), "/R/Download and clean AREMP Data.R"))

AREMP_data <- download_AREMP()

m <- AREMP_data%>% 
  leaflet() %>%
  addTiles() %>%
  addCircles(lng=~longitude, lat= ~lattitude,color="blue") 
       
m

```


#BLM AIM Data 
BLM published data yearly on an ArcGIS server here: https://landscape.blm.gov/geoportal/rest/find/document;jsessionid=F4427530B569FCCAEC19400F8C19E345?searchText=isPartOf%3AAIM&contentType=liveData&start=1&max=10&f=searchpage.
You can find the metadata on ScienceBase:

```{r}
#Open the BLM data and check the coordinate reference system of the BLM AIM data 
url <- list(hostname = "gis.blm.gov/arcgis/rest/services",
                                scheme = "https",
                                path = "hydrography/BLM_Natl_AIM_AquADat/MapServer/0/query",
                                query = list(
                                  where = "1=1",
                                  outFields = "*",
                                  returnGeometry = "true",
                                  f = "geojson")) %>% 
                      setattr("class", "url")
                    
                    request <- build_url(url)
                    BLM <- st_read(request, stringsAsFactors = TRUE) #Load the file from the Data file
                    st_crs(BLM)
                 
                    if(compareCRS(CRS, BLM)==TRUE){
                        print("BLM AIM coordinate reference system matches the coordinate system of the data exchange standards for the intergrated data set.")
                        } else{ 
                        print("BLM AIM coordinate reference system does not match the coordinate system of the data exchange standards for the intergrated data set.")
                       }
                   
                    BLM <- as_tibble(BLM)
  
  m <- BLM%>% 
      leaflet() %>%
      addTiles() %>%
      addCircles(lng=~MIDLONG, lat= ~MIDLAT, color="blue") 
  
  m
  
```
#USFS PIBO Dataset 

USFS does not published their data but it can be accessed via a data request by contacting: ADD CONTACT INFORMATION

You can find the metadata and the data used to create one dataset on ScienceBase:
 
NEED TO FIGURE OUT IF WE NEED TO UPDATE THE CORRDINATE SYSTEM OF THE PIBO LOCATION DATA 

```{r}
setwd("C:/Users/rscully/Documents/Projects/Habitat Data Sharing/2019_2020/Code/Intergrating-Stream-Monitoring-Data-From-Multiple-Programs")

PIBO <- read_xlsx(paste0(getwd(),"/data/2020_Seasonal_Sum_PIBO.xlsx"), 2)
  m <- PIBO%>% 
      leaflet() %>%
      addTiles() %>%
      addCircles(lng=~Long, lat= ~Lat, color="orange") 
  
  m

```

#Build an Intergrated DataSet 

```{r}
library(leaflet)
library(tidyverse)
setwd("C:/Users/rscully/Documents/Projects/Habitat Data Sharing/2019_2020/Code/Intergrating-Stream-Monitoring-Data-From-Multiple-Programs")
source(paste0(getwd(), "/R/Combind Data.R")) 
data <-integrate_data()



pal <- colorFactor(c("green","red","blue", "orange"), domain=c("1", "2", "3", "4"))

m <- data$Location%>% 
      leaflet() %>%
      addTiles() %>%
      addCircles(lng=~longitude, lat= ~latitude, color=~pal(datasetID)) %>% 
      addMarkers(clusterOptions = markerClusterOptions())

m



countEvents <- data$Event %>%
  group_by(Program)%>%
 count(Program) 


countLocations <- data$Location %>%
  group_by(datasetID)

#%>%
 # count(datasetID) 


#summary_df <- left_join(countLocations,countEvents, by="Program") 
#names(summary_df)[2]="Locations"
#names(summary_df)[3]="Events"

#write.csv(summary_df, paste0(getwd(), "/data/summary of events and locations.csv"))

#summary_df %>% 
 # kbl(col.names = c("Program", "Count of Locations", "Count of Events")) %>% 
  #  kable_styling()
  

#metrics <- data$Results %>% 
 #   group_by(measurementTerm) %>% 
  #  count(measurementTerm, sort=TRUE)

#write.csv(metrics, paste0(getwd(), "/data/summary of metrics.csv"))

```


The final data set is save to ScienceBase 


